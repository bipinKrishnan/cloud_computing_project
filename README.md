# Carbon Emission Estimator

## Table of Contents
- [About](#about)
- [Installation](#installation)
- [System Overview](#system-overview)
- [System Architecture](#system-architecture)
- [Application Overview](#application-overview)
- [Application Architecture](#application-architecture)
	 - [Flask App](#flask-app)
	 - [RESTful-API](#restful-api)
	 - [CRUD Operations](#crud-operations)
	 - [External APIs Used](#external-apis-used)
	 	- [Carbon Interface API](#carbon-interface-api)
- [Cloud Infrastructure](#cloud-infrastructure)
    - [Google Cloud](#google-cloud)
    - [Cloud Firestore](#cloud-datastore)
	  - [Kubernetes](#kubernetes)
	  - [Docker](#docker)



## About 

In today's world, climate change has become a major problem, and it has been observed that carbon dioxide emissions are primarily responsible for worsening the climate change. Therefore it becomes crucial to reduce CO2 emissions. The Carbon Emission Estimator allow users to estimate and track carbon emissions generated by vehicles. It uses carbon interface api to provide estimates.


### Pre-requisites:

Some familiarity with the below mentioned concepts:
- [Python 3.10]
- [Google Cloud Platform]()
- [Cloud Firestore]()
- [Carbon Interface API]()
- [Kubernetes]()
- [Docker]()


## Installation

1. Clone the repository

	```sh
	git clone https://github.com/thesouleddev/text-analyser.git
	```
2. Create virtual environment and install requirements with pip

	```sh
	python3 -m venv venv
	pip install -r requirements.txt
	```

3. Create Service Account on Google Cloud Console, we will need this service account json to setup and access Cloud Datastore. The account creation can be done by following the steps given in this [link](https://cloud.google.com/iam/docs/creating-managing-service-accounts).

	After creating the service account, download the service account json (sa.json) from console and move it to the application root directory. 

4. Create account on [Carbon Interface platform](https://www.carboninterface.com/), which is used for the carbon estimations.

6. Export environment variables

	```sh
	source local_env.sh
	```
7. Run the application locally
	```sh
	python main.py
	```

### File Structure 
File Structure of the project is given below
```
.
|──────text-analyser/
| |────modules/
| | |────text-analyser/
| | |──────__init__.py
| | |──────analyser.py
| | |──────base.py
| | |──────views.py
| |────models/
| | |────odb/
| | |──────__init__.py
| | |──────ndb.py
| | |────text_analyser.py/
| | |────users.py
| |────static/
| | |────styles/
| | |──────styles.css
| | |──────text-analyser.css
| |────template/
| | |────text_analyser/
| | |──────display.html
| | |──────document_render.html
| | |────header.html
| | |────.gitignore
| | |────Dockerfile
| | |────LICENSE
| | |────README.md
| | |────config.py
| | |────deployment.yaml
| | |────local_env.sh
| | |────main.py
| | |────requirements.txt
| | |────service.yaml
| | |────wsgi.py
| | |────sa.json
```

### Run flask for development
```
$ python main.py
```
### Run flask for production


```
$ gunicorn -w 4 -b 127.0.0.1:5000 run:app

```

* -w : number of worker
* -b : Socket to bind


### Run with Docker

```
$ docker build -t text-analyser .

$ docker run -p 5000:5000 --name text-analyser text-analyser
 
```

## System Overview

The user interface provides an intuitive and responsive experience. It communicates with the backend services to retrieve and display carbon emission data. The backend is responsible for communicating with database and fetching and updating all the vehicle information. It also utilizes the Carbon Interface API to fetch emission factors and calculate carbon emissions.

## System Architecture

![alt img](assets/sys_architecture.jpeg)

## Application Overview


### Main Page
This will be the initial dashboard page, where user will input the document inside the text box. Multiple analysis such as context, sentiment and toxicity can be generated by selecting the check box. 

![alt img](assets/sys_architecture.png)

After clicking the analyse button the report(s) are generated for the same.

### Context Analysis

![alt img](assets/context.png)

### Sentiment Analysis

![alt img](assets/sentiment.png)

### Toxicity Analysis

![alt img](assets/toxicity.png)

### Overall Result Page

![alt img](assets/result.png)


## Application Architecture

### Step 1 — Creating the test.py file: This file is used for creation of the flask app and running of the application.
  -  Intializing the database and setting configuration values using firebase_admin as given below



  - Following routes have been added
      - [Home Page](/)   
      - [Fetch and Display Page](/fetch_and_display): Responsible for fetching carbon emission details of a vehicle 
      - [Fetch Landing Page](/fetch_landing)
      - [Result Page](/result) : Shows the actual results values obtained
      - [Delete Landing Page](/del_landing)
      - [Delete Page](/delete) : Given the username, it deletes the vehicle details from the database


### Step 2 — Creating the support.py file: This file is used for calling the Carbon Interface APIs.
  -  get_vehicle_make_id: Creating the function get the vehicle_make_id for a specified vehicle.
      This fucntion calls the GET https://www.carboninterface.com/api/v1/vehicle_makes API which returns an array of vehcile makes for a specified vehicle

      Request

      ```
        curl "https://www.carboninterface.com/api/v1/vehicle_makes"
          -H "Authorization: Bearer API_KEY"
          -H "Content-Type: application/json"
          -X GET
      ```

      Response

      ```
        [
          {
            "data": {
              "id": "4c1e16e1-7967-4394-b3cb-15f4577dffa1",
              "type": "vehicle_make",
              "attributes": {
                "name": "Ferrari",
                "number_of_models": 243
              }
            }
          }  
        ]
      
       ```
      
  - get_vehicle_models_id: Once we get the vehicle_make_ids we create a function to get the vehicle_model_id 
      This function calls the GET https://www.carboninterface.com/api/v1/vehicle_make/<vehicle_make_id>/vehicle_models which takes vehicle_make_id as a parameter       and returns all the vehicle model belonging to the vehicle make


      Request

      ```
        curl "https://www.carboninterface.com/api/v1/vehicle_makes/2b1d0cd5-59be-4010-83b3-b60c5e5342da/vehicle_models"
          -H "Authorization: Bearer API_KEY"
          -H "Content-Type: application/json"
          -X GET
      ```

      Response
    
      ```
        [
          {
            "data": {
              "id": "7268a9b7-17e8-4c8d-acca-57059252afe9",
              "type": "vehicle_model",
              "attributes": {
                "name": "Corolla",
                "year": 1993,
                "vehicle_make": "Toyota"
              }
            }
        }
      ]
    ```

  - fetch_vehicle_emission: Once the vehicle model for which the estimations are to be performed is identified we use the vehicle_model_id of that model and call     the vehicle estimator API.
      Request
      ```
        curl "https://www.carboninterface.com/api/v1/estimates"
          -H "Authorization: Bearer API_KEY"
          -H "Content-Type: application/json"
          -X POST
          -d {
            "type": "vehicle",
            "distance_unit": "mi",
            "distance_value": 100,
            "vehicle_model_id": "7268a9b7-17e8-4c8d-acca-57059252afe9"
          }
      ```
      Response
      ```
        {
          "data": {
            "id": "6108d711-be04-4dc4-93f9-43d969fd5273",
            "type": "estimate",
            "attributes": {
              "distance_value": 100.0,
              "vehicle_make": "Toyota",
              "vehicle_model": "Corolla",
              "vehicle_year": 1993,
              "vehicle_model_id": "7268a9b7-17e8-4c8d-acca-57059252afe9",
              "distance_unit": "mi",
              "estimated_at": "2021-01-10T15:24:32.568Z",
              "carbon_g": 37029,
              "carbon_lb": 81.64,
              "carbon_kg": 37.03,
              "carbon_mt": 0.04
            }
          }
      }
    ```
### External APIs Used

#### Carbon Interface API

The Carbon Interface API provides details on the activity that is emitting carbon. The estimates API uses the most accurate estimation methodology to get the CO2 emissions.


#### Cloud Datastore

Why Google Cloud Datastore?

Datastore is a highly scalable NoSQL database. Datastore automatically handles sharding and replication, providing the application with a highly available and durable database that scales automatically to handle incoming load. Datastore provides a myriad of capabilities such as ACID transactions, SQL-like queries, indexes, and much more.

-   Product catalogs that provide real-time inventory and product details for a retailer.
-   User profiles that deliver a customised experience based on the user’s past activities and preferences.
-   Transactions based on ACID properties, for example, transferring funds from one bank account to another.

Datastore features include:

-   **Atomic transactions**. Datastore can execute a set of operations where either all succeed, or none occur.
-   **High availability of reads and writes**. Datastore runs in Google data centres, which use redundancy to minimize impact from points of failure.
-   **Massive scalability with high performance**. Datastore uses a distributed architecture to automatically manage scaling. Datastore uses a mix of indexes and query constraints so your queries scale with the size of your result set, not the size of your data set.
-   **Flexible storage and querying of data**. Datastore maps naturally to object-oriented and scripting languages, and is exposed to applications through multiple clients. It also provides a SQL-like query language.
-   **Balance of strong and eventual consistency**. Datastore ensures that entity lookups by key and ancestor queries always receive strongly consistent data. All other queries are eventually consistent. The consistency models allow your application to deliver a great user experience while handling large amounts of data and users.    
-   **Encryption at rest**. Datastore automatically encrypts all data before it is written to disk and automatically decrypts the data when read by an authorised user. For more information, see Server-Side Encryption.
-   **Fully managed with no planned downtime**. Google handles the administration of the Datastore service so you can focus on your application. Your application can still use Datastore when the service receives a planned upgrade.

### Kubernetes

Kubernetes is a portable, extensible, open source platform for managing containerised workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.

Kubernetes, at its basic level, is a system for running and coordinating containerised applications across a cluster of machines. It is a platform designed to completely manage the life cycle of containerised applications and services using methods that provide predictability, scalability, and high availability.

Kubernetes allows users to run scalable, highly available containerised workloads on a highly abstracted platform. While Kubernetes’ architecture and set of internal components can at first seem daunting, their power, flexibility, and robust feature set are unparalleled in the open-source world.

**service.yaml** contains Kubernetes load-balancer configuration for the project.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: text-analyer
spec:
  type: LoadBalancer
  selector:
    app: text-analyer
  ports:
    - port: 80
      targetPort: 8082
 ```

**deployment.yaml** contains Kubernetes nodes and replicas configuration for the project.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: text-analyer
  labels:
    name: text-analyer
spec:
  replicas: 2
  selector:
    matchLabels:
      name: text-analyer
  template:
    metadata:
      name: text-analyer
      labels:
        name: text-analyer
    spec:
      containers:
        - name: text-analyer
          image: gcr.io/eminent-enigma-364712/text-analyser:v1.0
          ports:
            - containerPort: 8082
  ```

### Docker

Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By taking advantage of Docker’s methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production.

Docker provides the ability to package and run an application in a loosely isolated environment called a container. The isolation and security allows you to run many containers simultaneously on a given host. Containers are lightweight and contain everything needed to run the application, so you do not need to rely on what is currently installed on the host. You can easily share containers while you work, and be sure that everyone you share with gets the same container that works in the same way.

Docker provides tooling and a platform to manage the lifecycle of your containers:

-   Develop your application and its supporting components using containers.
-   The container becomes the unit for distributing and testing your application.
-   When you’re ready, deploy your application into your production environment, as a container or an orchestrated service. This works the same whether your production environment is a local data centre, a cloud provider, or a hybrid of the two.











